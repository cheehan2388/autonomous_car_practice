The system implements a local reactive controller for path tracking using Proximal Policy Optimization (PPO), guided by a global path generated via cubic spline interpolation (serving as the trajectory for the agent to follow).1. Actor-Critic Architecture (PPO)The learning agent is split into two specialized neural networks, both utilizing a shared architecture of two hidden layers with 64 units and ReLU activations.Policy Network (The Actor): * Architecture: Input ($S_{dim}=14$) → Linear(64) → ReLU → Linear(64) → ReLU → Diagonal Gaussian Head.Action Space: Continuous control with $a_{dim}=1$. It outputs a mean ($\mu$) and uses a fixed standard deviation ($\sigma$) to define a Normal distribution for sampling actions.PPO Clipping: Implements a clipped objective with a default $\epsilon = 0.2$ to ensure stable policy updates by limiting the change ratio between new and old policies.Value Network (The Critic):Architecture: Input ($S_{dim}=14$) → Linear(64) → ReLU → Linear(64) → ReLU → Linear(1).Purpose: Predicts the expected return $V(s)$. It uses Generalized Advantage Estimation (GAE) with $\gamma=0.99$ and $\lambda=0.95$ to calculate high-quality advantage targets for the Actor.2. State Design ($S_{dim} = 14$)The state vector is carefully constructed to give the agent a "memory" of past movement and a "view" of the future path:Past Context (6 dimensions): The last two recorded poses (x, y, yaw) from the simulator history.Future Path (8 dimensions): Four look-ahead waypoints $(x, y)$ sampled at fixed intervals from the cubic spline path relative to the current nearest point.Normalization: Spatial coordinates are normalized by the environment width (600.0), and yaw is converted to radians to maintain a consistent input scale for the neural network.3. Reward Function DesignThe reward signal is a composite of distance error, heading alignment, and progress:$$Reward = 0.8 \cdot e^{-0.1 \cdot d_{min}} + 0.2 \cdot e^{-0.1 \cdot \Delta_{yaw}^2} + R_{progress}$$ComponentLogicPurposeDistance Error ($d_{min}$)Exponential decay based on distance to nearest path point.Encourages precise path following.Heading Error ($\Delta_{yaw}$)Exponential decay based on difference between vehicle and path yaw.Encourages alignment with path direction.Progress ($R_{progress}$)$+0.1$ if moving forward along index; $-1.0$ if moving backward.Penalizes going the wrong way or standing still.4. Path Planning & EnvironmentTrajectory Generation: Uses a 2D Cubic Spline to generate smooth, continuous $(x, y, yaw, curvature)$ trajectories from random anchor points.Simulation Types: Support for "Basic", "Differential Drive", or "Bicycle" kinematic models.Multi-Processing: Training is accelerated using a MultiEnv wrapper that runs 8 environments in parallel via Python subprocesses and Pipes.